{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Python libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
      "       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
      "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
      "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
      "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
      "       'fractal_dimension_se', 'radius_worst', 'diagnosis'],\n",
      "      dtype='object')\n",
      "dimension of data: (569, 23)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 23 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      569 non-null    int64  \n",
      " 1   radius_mean             569 non-null    float64\n",
      " 2   texture_mean            569 non-null    float64\n",
      " 3   perimeter_mean          569 non-null    float64\n",
      " 4   area_mean               569 non-null    float64\n",
      " 5   smoothness_mean         569 non-null    float64\n",
      " 6   compactness_mean        569 non-null    float64\n",
      " 7   concavity_mean          569 non-null    float64\n",
      " 8   concave points_mean     569 non-null    float64\n",
      " 9   symmetry_mean           569 non-null    float64\n",
      " 10  fractal_dimension_mean  569 non-null    float64\n",
      " 11  radius_se               569 non-null    float64\n",
      " 12  texture_se              569 non-null    float64\n",
      " 13  perimeter_se            569 non-null    float64\n",
      " 14  area_se                 569 non-null    float64\n",
      " 15  smoothness_se           569 non-null    float64\n",
      " 16  compactness_se          569 non-null    float64\n",
      " 17  concavity_se            569 non-null    float64\n",
      " 18  concave points_se       569 non-null    float64\n",
      " 19  symmetry_se             569 non-null    float64\n",
      " 20  fractal_dimension_se    569 non-null    float64\n",
      " 21  radius_worst            569 non-null    float64\n",
      " 22  diagnosis               569 non-null    int64  \n",
      "dtypes: float64(21), int64(2)\n",
      "memory usage: 102.4 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>...</td>\n",
       "      <td>2.866059</td>\n",
       "      <td>40.337079</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>0.372583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>...</td>\n",
       "      <td>2.021855</td>\n",
       "      <td>45.491006</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.017908</td>\n",
       "      <td>0.030186</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>0.483918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.692180e+05</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>...</td>\n",
       "      <td>1.606000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.060240e+05</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>...</td>\n",
       "      <td>2.287000</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>...</td>\n",
       "      <td>3.357000</td>\n",
       "      <td>45.190000</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>0.032450</td>\n",
       "      <td>0.042050</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.023480</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.980000</td>\n",
       "      <td>542.200000</td>\n",
       "      <td>0.031130</td>\n",
       "      <td>0.135400</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.052790</td>\n",
       "      <td>0.078950</td>\n",
       "      <td>0.029840</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
       "count  5.690000e+02   569.000000    569.000000      569.000000   569.000000   \n",
       "mean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   \n",
       "std    1.250206e+08     3.524049      4.301036       24.298981   351.914129   \n",
       "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
       "25%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   \n",
       "50%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   \n",
       "75%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   \n",
       "max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       symmetry_mean  ...  perimeter_se     area_se  smoothness_se  \\\n",
       "count     569.000000  ...    569.000000  569.000000     569.000000   \n",
       "mean        0.181162  ...      2.866059   40.337079       0.007041   \n",
       "std         0.027414  ...      2.021855   45.491006       0.003003   \n",
       "min         0.106000  ...      0.757000    6.802000       0.001713   \n",
       "25%         0.161900  ...      1.606000   17.850000       0.005169   \n",
       "50%         0.179200  ...      2.287000   24.530000       0.006380   \n",
       "75%         0.195700  ...      3.357000   45.190000       0.008146   \n",
       "max         0.304000  ...     21.980000  542.200000       0.031130   \n",
       "\n",
       "       compactness_se  concavity_se  concave points_se  symmetry_se  \\\n",
       "count      569.000000    569.000000         569.000000   569.000000   \n",
       "mean         0.025478      0.031894           0.011796     0.020542   \n",
       "std          0.017908      0.030186           0.006170     0.008266   \n",
       "min          0.002252      0.000000           0.000000     0.007882   \n",
       "25%          0.013080      0.015090           0.007638     0.015160   \n",
       "50%          0.020450      0.025890           0.010930     0.018730   \n",
       "75%          0.032450      0.042050           0.014710     0.023480   \n",
       "max          0.135400      0.396000           0.052790     0.078950   \n",
       "\n",
       "       fractal_dimension_se  radius_worst   diagnosis  \n",
       "count            569.000000    569.000000  569.000000  \n",
       "mean               0.003795     16.269190    0.372583  \n",
       "std                0.002646      4.833242    0.483918  \n",
       "min                0.000895      7.930000    0.000000  \n",
       "25%                0.002248     13.010000    0.000000  \n",
       "50%                0.003187     14.970000    0.000000  \n",
       "75%                0.004558     18.790000    1.000000  \n",
       "max                0.029840     36.040000    1.000000  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import and visualize dataset\n",
    "\n",
    "data = pd.read_csv('bdata.csv')\n",
    "print(data.columns)\n",
    "print(\"dimension of data: {}\".format(data.shape))\n",
    "\n",
    "#No nulls and nothing out of the ordinary with info() results\n",
    "data.info()\n",
    "data.head()\n",
    "\n",
    "# , 'area_mean', 'smoothness_mean', 'compactness_mean'\n",
    "X = data[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
    "       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "       'fractal_dimension_se']]\n",
    "y = data['diagnosis']\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                        False\n",
       "radius_mean               False\n",
       "texture_mean              False\n",
       "perimeter_mean            False\n",
       "area_mean                 False\n",
       "smoothness_mean           False\n",
       "compactness_mean          False\n",
       "concavity_mean            False\n",
       "concave points_mean       False\n",
       "symmetry_mean             False\n",
       "fractal_dimension_mean    False\n",
       "radius_se                 False\n",
       "texture_se                False\n",
       "perimeter_se              False\n",
       "area_se                   False\n",
       "smoothness_se             False\n",
       "compactness_se            False\n",
       "concavity_se              False\n",
       "concave points_se         False\n",
       "symmetry_se               False\n",
       "fractal_dimension_se      False\n",
       "radius_worst              False\n",
       "diagnosis                 False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensure data does not contain blanks\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training set for supervised learning models\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Neural networks also expect all input features to vary in a similar way, and ideally to have a mean of 0, and a variance of 1.\n",
    "#Rescale our data to fulfills these requirements.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "mlp = MLPClassifier(random_state=0)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    mlp.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n",
    "\n",
    "\n",
    "#Confusion Matrix\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "# Precision/Recall\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "NN_roc_auc = roc_auc_score(y_test, mlp.predict(X_test_scaled))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, mlp.predict_proba(X_test_scaled)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='NN' % NN_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('RF_ROC')\n",
    "plt.show()\n",
    "\n",
    "# calculate AUC\n",
    "\n",
    "print('AUC: %.3f' % metrics.auc(fpr, tpr))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Problem: The goal of this assignment is to accurately predict whether a patients breast cancer tumor is malignant or benign. With this second opinion, doctors can better server our community by treating patients who have malignant tumors. The model should assist doctors in determining whether treatment is needed based on tumor classification.\n",
    "\n",
    "Machine Learning Applications: Using a Naive Bayes to read chest X-Rays to determine if a patient has COVID, pneumonia, or normal scans. Second example is using K-Means clustering to read Alzheimer's patient data to search for commonalities between patients. Third example is using logistic regression to classify a patient's diabetes as type 1 or 2.\n",
    "\n",
    "Machine Learning Algorithms: \n",
    "Naive Bayes:\n",
    "Using Bayes' Theorem, we can determine which class an object belongs to and use its features to predict future objects. Naive Bayes also assumes that all attributes of a data record are independent. This algorithm is great for its speed and ability to slice through high dimensions of data like text and email. Unfortunately, assuming all attributes are independent is not something that happens often in real life scenarios.\n",
    "\n",
    "Logistic Regression:\n",
    "Logistic regression differs from linear regression in a few ways. Linear regression can be used to predict any values along a straight line, however, logistic regression tries to predict values between 0 and 1. As such, we use Logistic regression for classification problems where we are trying to determine 0 and 1. Using thresholds, we can determine the class an object can fall into. Just like Naive Bayes, Linear regression is simple and can provide results with great speed and limited processing powers. Unfortunately, Linear regression requires a large focus on data quality and ensuring no missing values exist in the dataset. The algorithm does not handle missing data very well.\n",
    "\n",
    "Decision Trees:\n",
    "The goal of a decision tree algorithm is to predict the value of a target variable by learning simple decision rules inferred from the data features. The tree portion is the different questions or qualifications that lead you to a corresponding leaf. The leaf is the classification outcome expected from the model. Decision trees are very simple and easy to understand, however they are not very stable as any change to the data can cause negative impacts to the model.\n",
    "\n",
    "K-Means:\n",
    "A K-Means model uses the sum of squares to group data into centroids. The model will work to group simliar datapoints together and point out patterns. Simple, flexible and great for a large dataset. However, it is memory intensive and requires a lot of computing resources.\n",
    "\n",
    "Data Preprocessing Discussion: I took a look at the dataset to see how many rows and columns we had. Knowing that, I took it a step further to see if any of the data points were blank. Lastly, I ensured the datapoints made sense by checking for extreme outliers. An example of that would be looking for a value of over 1000 because a tumor shouldn't be that large.\n",
    "\n",
    "Explaining Metrics: In the metrics section, I was mostly focused on ensuring we looked at each of the features. Understanding their importance is key in understanding how the model is functioning with the data. I used importance scores to rank each of the features.\n",
    "\n",
    "Interpreting Results: I used accuracy calculation, ROC/AUG values, and the confusion matrix to evaluate the models performance. Taking into account that we are in the healthcare industry and lives are at stake, maximizing performance while ensuring the least amount of false negatives were created was paramount. False positives leave patients with life threatening tumors believing they are safe and sound when they are in need of treatment. Based on the ROC/AUG and false positive values, I determined Naive Bayes model performed the best.\n",
    "\n",
    "Recommended Steps: The first step in optimizing performance, in my opinion, would be to use the feature importance values calculated in the explain metrics section to limit the amount of features fed to the model. Next, I would tune the models to ensure we had virtually 0 false negatives to ensure patient safety."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
